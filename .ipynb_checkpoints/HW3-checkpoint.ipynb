{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE327 Homework 3\n",
    "**Due date: 11:59 pm on Nov. 16, 2017 (Thursday)**\n",
    "\n",
    "## Description\n",
    "---\n",
    "In this homework, we will examine the task of scene recognition starting with very simple methods — tiny images and nearest neighbor classification — and then move on to more advanced methods — bags of quantized local features and linear classifiers learned by support vector machines.\n",
    "\n",
    "Bag of words models are a popular technique for image classification inspired by models used in natural language processing. The model ignores or downplays word arrangement (spatial information in the image) and classifies based on a histogram of the frequency of visual words. The visual word \"vocabulary\" is established by clustering a large corpus of local features. See Szeliski chapter 14.4.1 for more details on category recognition with quantized features. In addition, 14.3.2 discusses vocabulary creation and 14.1 covers classification techniques.\n",
    "\n",
    "For this homework you will be implementing a basic bag of words model. You will classify scenes into one of 15 categories by training and testing on the 15 scene database (introduced in [Lazebnik et al. 2006](http://www.di.ens.fr/willow/pdfs/cvpr06b.pdf), although built on top of previously published datasets). [Lazebnik et al. 2006](http://www.di.ens.fr/willow/pdfs/cvpr06b.pdf) is a great paper to read, although we will be implementing the baseline method the paper discusses (equivalent to the zero level pyramid) and not the more sophisticated spatial pyramid (which is extra credit). For an excellent survey of pre-deep-learning feature encoding methods for bag of words models, see [Chatfield et al, 2011](http://www.robots.ox.ac.uk/~vgg/research/encoding_eval/).\n",
    "\n",
    "You are required to implement 2 different image representations -- tiny images and bags of SIFT features -- and 2 different classification techniques -- nearest neighbor and linear SVM. In the end of the notebook, you are specifically asked to report performance for the following combinations, and it is also highly recommended that you implement them in this order:\n",
    "\n",
    "- Tiny images representation and nearest neighbor classifier (accuracy of about 18-25%).\n",
    "- Bag of SIFT representation and nearest neighbor - classifier (accuracy of about 50-60%).\n",
    "- Bag of SIFT representation and linear SVM classifier (accuracy of about 60-70%).\n",
    "\n",
    "## Dataset\n",
    "---\n",
    "![Dataset Examples](https://www.cc.gatech.edu/~hays/compvision/proj4/categories.png \"Example scenes from of each category in the 15 scene dataset. Figure from Lazebnik et al. 2006.\")\n",
    "The starter code trains and tests on 100 images from each category (i.e. 1500 training examples total and 1500 test cases total). In a real research paper, one would be expected to test performance on random splits of the data into training and test sets, but the starter code does not do this to ease debugging. Download the dataset [here](https://drive.google.com/a/cs.stonybrook.edu/file/d/0B446EB1iI6_Qc0Q1NTRTajdUVTg/view?usp=sharing).\n",
    "\n",
    "## Starter Code\n",
    "---\n",
    "To make your task a little easier, below we provide some starter code which randomly guesses the category of every test image and achieves about 7% accuracy (1 out of 15 guesses is correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages here\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of my dummy model is 6.53%\n"
     ]
    }
   ],
   "source": [
    "class_names = [name[11:] for name in glob.glob('data/train/*')]\n",
    "class_names = dict(zip(xrange(len(class_names)), class_names))\n",
    "\n",
    "def load_dataset(path, num_per_class=-1):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for id, class_name in class_names.iteritems():\n",
    "        img_path_class = glob.glob(path + class_name + '/*.jpg')\n",
    "        if num_per_class > 0:\n",
    "            img_path_class = img_path_class[:num_per_class]\n",
    "        labels.extend([id]*len(img_path_class))\n",
    "        for filename in img_path_class:\n",
    "            data.append(cv2.imread(filename, 0))\n",
    "    return data, labels\n",
    "\n",
    "# load training dataset\n",
    "train_data, train_label = load_dataset('data/train/')\n",
    "train_num = len(train_label)\n",
    "\n",
    "# load testing dataset\n",
    "test_data, test_label = load_dataset('data/test/', 100)\n",
    "test_num = len(test_label)\n",
    "\n",
    "# feature extraction\n",
    "def extract_feat(raw_data):\n",
    "    feat_dim = 1000\n",
    "    feat = np.zeros((len(raw_data), feat_dim), dtype=np.float32)\n",
    "    for i in xrange(feat.shape[0]):\n",
    "        feat[i] = np.reshape(raw_data[i], (raw_data[i].size))[:feat_dim] # dummy implemtation\n",
    "        \n",
    "    return feat\n",
    "\n",
    "train_feat = extract_feat(train_data)\n",
    "test_feat = extract_feat(test_data)\n",
    "\n",
    "# model training: take feature and label, return model\n",
    "def train(X, Y):\n",
    "    return 0 # dummy implementation\n",
    "\n",
    "# prediction: take feature and model, return label\n",
    "def predict(model, x):\n",
    "    return np.random.randint(15) # dummy implementation\n",
    "\n",
    "# evaluation\n",
    "predictions = [-1]*len(test_feat)\n",
    "for i in xrange(test_num):\n",
    "    predictions[i] = predict(None, test_feat[i])\n",
    "    \n",
    "accuracy = sum(np.array(predictions) == test_label) / float(test_num)\n",
    "\n",
    "print \"The accuracy of my dummy model is {:.2f}%\".format(accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Tiny Image Representation + Nearest Neighbor Classifier\n",
    "{25 points} You will start by implementing the tiny image representation and the nearest neighbor classifier. They are easy to understand, easy to implement, and run very quickly for our experimental setup (less than 10 seconds).\n",
    "\n",
    "The \"tiny image\" feature is one of the simplest possible image representations. One simply resizes each image to a small, fixed resolution (we recommend 16x16). It works slightly better if the tiny image is made to have zero mean and unit length (normalization). This is not a particularly good representation, because it discards all of the high frequency image content and is not especially invariant to spatial or brightness shifts. We are using tiny images simply as a baseline.\n",
    "\n",
    "The nearest neighbor classifier is equally simple to understand. When tasked with classifying a test feature into a particular category, one simply finds the \"nearest\" training example (L2 distance is a sufficient metric) and assigns  the label of that nearest training example to the test example. The nearest neighbor classifier has many desirable features — it requires no training, it can learn arbitrarily complex decision boundaries, and it trivially supports multiclass problems. It is quite vulnerable to training noise, though, which can be alleviated by voting based on the K nearest neighbors (but you are not required to do so). Nearest neighbor classifiers also suffer as the feature dimensionality increases, because the classifier has no mechanism to learn which dimensions are irrelevant for the decision.\n",
    "\n",
    "**Hints**:\n",
    "- Use [cv2.resize()](https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html#resize) to resize the images;\n",
    "- Use [NearestNeighbors in Sklearn](http://scikit-learn.org/stable/modules/neighbors.html) as your nearest neighbor classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Bag of SIFT Representation + Nearest Neighbor Classifer\n",
    "{40 points} After you have implemented a baseline scene recognition pipeline it is time to move on to a more sophisticated image representation — bags of quantized SIFT features. Before we can represent our training and testing images as bag of feature histograms, we first need to establish a vocabulary of visual words. We will form this vocabulary by sampling many local features from our training set (10's or 100's of thousands) and then cluster them with k-means. The number of k-means clusters is the size of our vocabulary and the size of our features. For example, you might start by clustering many SIFT descriptors into k=50 clusters. This partitions the continuous, 128 dimensional SIFT feature space into 50 regions. For any new SIFT feature we observe, we can figure out which region it belongs to as long as we save the centroids of our original clusters. Those centroids are our visual word vocabulary. Because it can be slow to sample and cluster many local features, the starter code saves the cluster centroids and avoids recomputing them on future runs.\n",
    "\n",
    "Now we are ready to represent our training and testing images as histograms of visual words. For each image we will densely sample many SIFT descriptors. Instead of storing hundreds of SIFT descriptors, we simply count how many SIFT descriptors fall into each cluster in our visual word vocabulary. This is done by finding the nearest neighbor k-means centroid for every SIFT feature. Thus, if we have a vocabulary of 50 visual words, and we detect 220 distinct SIFT features in an image, our bag of SIFT representation will be a histogram of 50 dimensions where each bin counts how many times a SIFT descriptor was assigned to that cluster. The total of all the bin-counts is 220. The histogram should be normalized so that image size does not dramatically change the bag of features magnitude.\n",
    "\n",
    "**Note**: \n",
    "- Instead of using SIFT to detect invariant keypoints which is time-consuming, you are recommended to densely sample keypoints in a grid with certain step size (sampling density) and scale.\n",
    "- There are many design decisions and free parameters for the bag of SIFT representation (number of clusters, sampling density, sampling scales, SIFT parameters, etc.) so accuracy might vary from 50% to 60%.\n",
    "\n",
    "**Hints**:\n",
    "- Use [KMeans in Sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) to do clustering and find the nearest cluster centroid for each SIFT feature;\n",
    "- Use `cv2.xfeatures2d.SIFT_create()` to create a SIFT object;\n",
    "- Use `sift.compute()` to compute SIFT descriptors given densely sampled keypoints ([cv2.Keypoint](https://docs.opencv.org/3.0-beta/modules/core/doc/basic_structures.html?highlight=keypoint#keypoint))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Bag of SIFT Representation + one-vs-all SVMs\n",
    "{15 points} The last task is to train one-vs-all linear SVMS to operate in the bag of SIFT feature space. Linear classifiers are one of the simplest possible learning models. The feature space is partitioned by a learned hyperplane and test cases are categorized based on which side of that hyperplane they fall on. Despite this model being far less expressive than the nearest neighbor classifier, it will often perform better.\n",
    "\n",
    "You do not have to implement the support vector machine. However, linear classifiers are inherently binary and we have a 15-way classification problem (the library has handled it for you). To decide which of 15 categories a test case belongs to, you will train 15 binary, one-vs-all SVMs. One-vs-all means that each classifier will be trained to recognize 'forest' vs 'non-forest', 'kitchen' vs 'non-kitchen', etc. All 15 classifiers will be evaluated on each test case and the classifier which is most confidently positive \"wins\". E.g. if the 'kitchen' classifier returns a score of -0.2 (where 0 is on the decision boundary), and the 'forest' classifier returns a score of -0.3, and all of the other classifiers are even more negative, the test case would be classified as a kitchen even though none of the classifiers put the test case on the positive side of the decision boundary. When learning an SVM, you have a free parameter $\\lambda$ (lambda) which controls how strongly regularized the model is. Your accuracy will be very sensitive to $\\lambda$, so be sure to try many values.\n",
    "\n",
    "After evaluation, compute and draw a (normalized) [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) for your final model, here is an [example](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py).\n",
    "\n",
    "**Bonus Part 1 {15 points}**: One drawback of Bag of Visual Words is, all local features are encoded into a single code vector ignoring the position of the feature descriptors, which means spatial information between words are discarded in the final code vector. Thus, to incorporate the spatial information into the final code vector, one can apply Spatial Pyramid Matching, a very simple but powerful idea proposed in [Lazebnik et al. 2006](http://www.di.ens.fr/willow/pdfs/cvpr06b.pdf).\n",
    "\n",
    "**Bonus Part 2 {10 points}**: Another 10 points will be given to students whose accuracy ranks top 3 in this homework. Don't cheat and don't train your model on testing data, I will run your code through with a separate testing dataset!\n",
    "\n",
    "**Hints**:\n",
    "- Use SVM in [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm) (recommended) or [OpenCV](https://docs.opencv.org/3.0-alpha/modules/ml/doc/support_vector_machines.html) to do training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Report\n",
    "---\n",
    "{20 points} Please report the performance of different approaches you used in each problem and describe your algorithm and any decisions you made to write your algorithm in a particular way. Report the performance of your method and discuss how different choices you made affect it. Report the confusion matrix requested in problem 3, and discuss where the method performs best and worse. Discuss any extra credit you did and show what contribution it had on the results (e.g. performance with and without the extra credit component)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Write your report here in markdown or html-->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
